通常，在数据清洗完成，输入模型之前，需要对分类变量进行编码。目前我所知道的编码方式有以下几种：
One-hot 、 Dummy 、Ordinal 、Binary等 

编码：树模型通常不需要进行编码（进行One-hot增加了维度，相当于人为的增加了树的深度），只有计算距离的模型才需要进行编码。One-hot编码就是为了防止无序的变量取值在编码之后变得有序。

## One-hot VS Dummy:
**参考：**<br>
- [https://mp.weixin.qq.com/s?__biz=MzI3NTA0MzM1OQ==&mid=2651615354&idx=1&sn=b74b1d526746b401947ff98b389e797f&chksm=f0f2150ac7859c1c2ebd41f080ff40b034443118d0c9d27a3964a59b7ad8c8cfa232346d8262&mpshare=1&scene=1&srcid=0527OgF5sinRLzc8j2qdc2Xm#rd](https://mp.weixin.qq.com/s?__biz=MzI3NTA0MzM1OQ==&mid=2651615354&idx=1&sn=b74b1d526746b401947ff98b389e797f&chksm=f0f2150ac7859c1c2ebd41f080ff40b034443118d0c9d27a3964a59b7ad8c8cfa232346d8262&mpshare=1&scene=1&srcid=0527OgF5sinRLzc8j2qdc2Xm#rd)<br>
-[http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/) <br> 
- [http://www.statsmodels.org/devel/contrasts.html](http://www.statsmodels.org/devel/contrasts.html) <br>
- [http://cml.ics.uci.edu/](http://cml.ics.uci.edu/)

----------

One-hot将每个变量的每个取值展成一个维度，可能导致维度爆炸(好处也是人为的扩充了维度，且相当于进行了一次kernel的映射，可能可以解决线性不可分问题？),如果训练集的量不变，过高的维度可能不利于模型训练<br>

1. 此时可以做PCA进行降维
2. ordinal coding : 但会人为引入无关类别取值的顺序
2. 可以先对变量的取值进行聚类，再进行编码
3. 正则化
4. 但这种方式最大的问题在于，One-hot 编码会导致多重线性问题，使方程有多组解，进而造成模型不稳定；或者得到的权值因子不对（即，权值较大的变量，实际对结果的影响并不大，还是因为受到多组解的影响）。
5. Dummy可以通过减少一维（k-1维）对变量取值进行编码，避免这个问题。但这样，当测试集中出现了训练集中没有出现过的取值是，无法处理。One-hot可以全取0来应对这种情况。
6. 此时就可以另外一种方式来理解正则化：因为One-hot有多重共线的问题，因而会有很多组解，正则化就是为了抑制解空间。 


----------

## 类别型变量编码方式汇总
（参考： [http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)）

1. Ordinal:每种类型取值给一个值（1，2，3...），相当于人为的引入了类别取值之间本不存在的顺序关系。但是，有一些变量，比如年龄（幼儿、青年、中年、老年）这种，有序的类别，可以试试这种方式。
2. One-hot：不再赘述
3. Dummy（Treament）:![](https://i.imgur.com/PgGN8kS.png)（[https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn)）
3. Binary:先进行 Ordinal 编码，在将每个取值二进制化，再把二进制串的每一位，当成一个新的维度。我认为这种方法是 Ordinal 和 Onh-hot的一个折中。相比于Ordinal，Binary将类别顺序打乱了；但相比于One-hot，Binary的维度又降低了。
4. Sum(Deviation)：
5. Backward Difference Coding:
6. Helmert Coding:
7. Orthogonal Polynomical Coding:
8. User-defined Coding:

各种编码在UCI公共数据集上的效果：
![](https://i.imgur.com/QAp27hG.png)

![](https://i.imgur.com/ArLrJAw.png)

![](https://i.imgur.com/k8WBgOX.png)

从上面的结果看起来看起来，Ordinal的效果一直比较差；Binary Coding的效果还不错，One-hot时好时坏。

----------

## scikit-learn 中的特征二值化方式
直接引用：[https://mp.weixin.qq.com/s?__biz=MzI3NTA0MzM1OQ==&mid=2651615335&idx=1&sn=d3910d6b12121e040a54f4c73bde4202&mpshare=1&scene=1&srcid=05276w2L1RFZx8t6BtsprqO7#rd](https://mp.weixin.qq.com/s?__biz=MzI3NTA0MzM1OQ==&mid=2651615335&idx=1&sn=d3910d6b12121e040a54f4c73bde4202&mpshare=1&scene=1&srcid=05276w2L1RFZx8t6BtsprqO7#rd)



----------


## 多重共线问题：
参考：[https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/model-assumptions/multicollinearity-in-regression/](https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/model-assumptions/multicollinearity-in-regression/) <br>
主要是指在**基于最小二乘法的多元线性回归中（为什么）**，因变量直接相关的情况。当多重共线性问题比较严重的时候，会导致多解，从而造成回归系数的方差很大，模型不稳定，并可能导致如下问题。
- 强相关的自变量之间，回归系数差异很大；
- 自变量与因变量强相关，但回归系数可能很小；
- 删除强相关的自变量中的一个，会严重影响其他多项自变量的回归系数。
- 出现了相关系数与回归方程系数符号相反的问题

**如何判定：**<br>
- 检查自变量的相关性结构，如 One-hot 编码，就会导致严重的多重共线性问题。如果相关性系数较高（0.8），则表明存在多重共线性问题；但相关系数低，并不能表示不存在多重共线性（可能多组变量的组合之间存在强相关）<br>
- 查看方差膨胀因子（VIF）：VIF 用于在您的预测变量相关时，度量估计回归系数的方差增加的幅度。如果所有 VIF 都为 1，则不存在多重共线性，但如果有些 VIF 大于 1，则预测变量为相关。VIF 大于 5 时，该项的回归系数的估计结果不理想。**多重共线性不会影响拟合优度和预测优度。系数（线性判别函数）无法可靠地进行解译，但拟合（分类）值不会受到影响。** 是<br>
- 发现系数估计值的符号不对<br>
- 某些重要的解释变量t值低，而R方不低<br>
- 当一不太重要的解释变量被删除后，回归结果显著变化<br>


**解决方法**<br>
https://support.minitab.com/zh-cn/minitab/18/help-and-how-to/modeling-statistics/regression/supporting-topics/model-assumptions/multicollinearity-in-regression/
- 如果要对多项式进行拟合，请将预测变量值减去预测变量的均值
- 从模型中删除那些高度相关的预测变量。由于它们提供了冗余信息，因此删除它们通常不会显著减少 R2。考虑使用逐步回归、最佳子集回归或数据集的专门知识来删除这些变量。
- 使用偏最小二乘或主成分分析。这些方法可以将预测变量的数量减少为更小的不相关分量集。
- 增加数据集
- 对模型加约束：正则化（L2 岭回归）
- PCA



**处理多重共线性的原则：**
- 多重共线性是普遍存在的，轻微的多重共线性问题可不采取措施；
-  严重的多重共线性问题，一般可根据经验或通过分析回归结果发现。如影响系数符号，重要的解释变量t值很低。要根据不同情况采取必要措施。
-  如果模型仅用于预测，则只要拟合程度好，可不处理多重共线性问题，存在多重共线性的模型用于预测时，往往不影响预测结果；



PS： 这块水比较深，详挖的话东西比较多，先占个坑，等我先补完机器学习的基础东西之后，再来填坑。。